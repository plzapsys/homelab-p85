{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pando85's Homelab","text":"<p>This project utilizes Infrastructure as Code and GitOps to automate provisioning, operating, and updating self-hosted services in my homelab. Based in K3s, ArgoCD, Renovate and ZFS. It can be used as a highly customizable framework to build your own homelab.</p> <p>What is a homelab?</p> <p>Homelab is a laboratory at home where you can self-host, experiment with new technologies, practice for certifications, and so on. For more information about homelab in general, see the r/homelab introduction.</p>"},{"location":"#overview","title":"\ud83d\udcd6 Overview","text":"<p>This section provides a high level overview of the project. For further information, please see the documentation.</p>"},{"location":"#kubernetes","title":"\u26f5 Kubernetes","text":"<p>This repo is focused in maintain in a GitOps practical way my home infrastructure. Ansible is used to deploy a simple K3s cluster. Managed by ArgoCD.</p>"},{"location":"#installation","title":"Installation","text":"<p>The cluster is running on Debian based distributions, deployed on bare-metal. We use custom Ansible playbooks and roles to setup the Kubernetes cluster.</p>"},{"location":"#core-components","title":"Core components","text":"<ul> <li>external-secrets: External Secrets   Operator reads information from a Vault and automatically injects the values as Kubernetes   Secrets.</li> <li>hashicorp/vault: A tool for secrets management, encryption as a   service, and privileged access management.</li> <li>kubernetes-sigs/external-dns: Automatically   manages DNS records from my cluster in a cloud DNS provider.</li> <li>jetstack/cert-manager: Creates SSL certificates for services in   my Kubernetes cluster.</li> <li>kubernetes/ingress-nginx: Ingress controller to   expose HTTP traffic to pods over DNS.</li> <li>openebs/zfs-localpv: CSI Driver for dynamic   provisioning of Persistent Local Volumes for Kubernetes using ZFS.</li> <li>kanidm: A simple, secure and fast identity management platform.</li> <li>velero: Tool to safely backup and restore, perform disaster recovery,   and migrate Kubernetes cluster resources and persistent volumes.</li> </ul>"},{"location":"#hardware","title":"\ud83d\udd27 Hardware","text":"Hostname Device Count OS Disk Size Data Disk Size Ram Operating System Purpose grigri Supermicro Atom C2758 (A1SRi-2758F) 1 250GB SSD 3*4TB + 500GB (NVMe) RAIDZ + cache 32GB Ubuntu 22.04 K3s server k8s-amd64-1* AMD E-450 APU 1 60GB N/A 8GB Ubuntu 22.04 k3s agent k8s-rock64-i Rock64 6 N/A N/A 4GB Armbian K3s agent k8s-odroid-c4-i Odroid-c4 2 N/A N/A 4GB Armbian K3s agent k8s-odroid-hc4-i Odroid-hc4 1 N/A 3TB + 240GB SSD 4GB Armbian K3s agent pfsense PC Engines APU2e4 1 60GB N/A 4GB pfSense/FreeBSD Router gs724t Netgear gs724t 1 N/A N/A N/A N/A Switch cerezo Unifi UAP 1 N/A N/A N/A N/A AP manzano Unifi UAP 1 N/A N/A N/A N/A AP <p>* with Nvidia GeForce GTX 1060 3GB</p>"},{"location":"#images","title":"Images","text":""},{"location":"#features","title":"\u2b50 Features","text":"<ul> <li> Common applications: Jellyfin, Gitea, arr, Nextcloud...</li> <li> Automated Kubernetes installation and management</li> <li> Installing and managing applications using GitOps</li> <li> Automatic rolling upgrade for OS and Kubernetes</li> <li> Automatically update apps (with approval if needed)</li> <li> Modular architecture, easy to add or remove features/components</li> <li> Automated certificate management</li> <li> Automatically update DNS records for exposed services</li> <li> Monitoring and alerting</li> <li> Single sign-on</li> <li> Automated backups</li> </ul>"},{"location":"#dns","title":"\ud83c\udf10 DNS","text":"<p>ExternalDNS is deployed in the cluster and configured to sync DNS records to Cloudflare.</p> <p>All connections outside the cluster are handled with TLS using cert-manager with Let's Encrypt.</p>"},{"location":"#load-balancer","title":"Load Balancer","text":"<p>Cilium is configured with BGP control plane, both on my router and within the Kubernetes cluster.</p>"},{"location":"#ingress-controllers","title":"Ingress Controllers","text":"<p>For external access, port forwarding is configured for ports <code>80</code> and <code>443</code>, directing traffic to the load balancer IP of the Kubernetes ingress controller.</p> <p>There are also another ingress controller for internal use.</p>"},{"location":"#internal-dns","title":"Internal DNS","text":"<p><code>internal.grigri.cloud</code> domain is used. Configured as:</p> <pre><code>annotations:\ncert-manager.io/cluster-issuer: letsencrypt-prod-dns\nexternal-dns.alpha.kubernetes.io/enabled: \"true\"\n</code></pre>"},{"location":"#external-dns","title":"External DNS","text":"<p><code>grigri.cloud</code> domain is used. Configured as:</p> <pre><code>annotations:\ncert-manager.io/cluster-issuer: letsencrypt-prod-dns\nexternal-dns.alpha.kubernetes.io/enabled: \"true\"\nexternal-dns.alpha.kubernetes.io/target: grigri.cloud\n</code></pre>"},{"location":"#thanks","title":"\ud83e\udd1d Thanks","text":"<p>Thanks to all folks who donate their time to the Kubernetes @Home community. A lot of inspiration for my cluster came from those that have shared their clusters over at awesome-home-kubernetes.</p>"},{"location":"deployment/cilium-bgp-control-plane/","title":"Cilium BPG control plane","text":"<p>Cilium replaces MetalLB for creating K8s LBs based on BGP.</p>"},{"location":"deployment/cilium-bgp-control-plane/#deployment","title":"Deployment","text":"<p>Ansible set up the configuration and ArgoCD deploys Cilium.</p>"},{"location":"deployment/cilium-bgp-control-plane/#router-config","title":"Router config","text":"<p>In order to use Cilium BGP control plane mode we must configure Pfsense as router to be able of share BGP route table and route all network to that load balancer IPs. We use this tutorial</p> <ul> <li> <p>install package ffr</p> </li> <li> <p>configure <code>Services-&gt;FRR-&gt;Global Settings</code>:</p> </li> </ul> <pre><code>[general options]\nenable=x\ndefault_router_id=192.168.192.1\n</code></pre> <ul> <li><code>Services-&gt;FRR-&gt;Global Settings-&gt;Route Maps</code>:</li> </ul> <pre><code>- name: allow-all\ndescription: Match any route\naction: permit\nSequence: 100\n</code></pre> <ul> <li><code>Services-&gt;FRR-&gt;BGP-&gt;BGP</code>:</li> </ul> <pre><code>[bgp router options]\nenable=x\nlocal_as=64512\n\n[graceful restart/shutdown]\nenable_bgp_graceful_shutdown=true\n</code></pre> <ul> <li>in <code>Services-&gt;FRR-&gt;BGP-&gt;Neighbors</code>:</li> </ul> <pre><code>- name: 192.168.192.2\ndescr: grigri\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.11\ndescr: k8s-amd64-1\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.23\ndescr: k8s-odroid-hc4-3\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.31\ndescr: k8s-odroid-c4-1\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.32\ndescr: k8s-odroid-c4-2\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.41\ndescr: k8s-rock64-1\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.42\ndescr: k8s-rock64-2\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.43\ndescr: k8s-rock64-3\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.44\ndescr: k8s-rock64-4\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.45\ndescr: k8s-rock64-5\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n- name: 192.168.192.46\ndescr: k8s-rock64-6\nremote_as: 64513\nnext_hop_self: true\nroute_map_filters:\ninbound_router_map_filter: allow-all\noutbound_router_map_filter: allow-all\nallow_as_inbound: enabled\n</code></pre> <p>Important: to access Cilium IP pools network from kubernetes subnet you need to add your host to bgp Some issues could be experimented if not added as <code>docker push</code> not working correctly.</p>"},{"location":"deployment/manual-setup/","title":"Manual setup","text":"<ul> <li>Network</li> <li>Pfsense</li> <li>Servers</li> <li>Flash SDs<ul> <li>amd64 instances</li> <li>odroid-hc4</li> <li>Bootloader Bypass Method</li> <li>Naming convention</li> </ul> </li> <li>Troubleshooting<ul> <li>Same mac problem</li> <li>No python interpreter found</li> </ul> </li> <li>Setup</li> <li>Stability problems (currently included 400MHz)</li> <li>Cluster</li> </ul>"},{"location":"deployment/manual-setup/#network","title":"Network","text":""},{"location":"deployment/manual-setup/#pfsense","title":"Pfsense","text":"<ul> <li>Connect to DMZ <code>192.168.192.0/24</code></li> <li>Add DHCP server: range(60-99), but fix agent IPs before add to the cluster.</li> <li>only for controller HA - Create lb for apiserver (used HaProxy: increase client, server and   tunnel* timeouts to 86400000)</li> <li>Add DNS entry</li> </ul> <p>tunnel*: must be added in <code>backend-&gt;advanced settings-&gt;backend pass thru</code> as <code>timeout tunnel 86400s</code></p>"},{"location":"deployment/manual-setup/#servers","title":"Servers","text":""},{"location":"deployment/manual-setup/#flash-sds","title":"Flash SDs","text":"<ul> <li>odroid-c4: image</li> <li>odroid-hc4: image</li> <li>amd64: usb-stick</li> <li>grigri: usb-stick</li> <li>prusik-ipmi: image</li> </ul> <p>Use script from <code>scripts/prepare_sdcard.sh</code> to prepare instances. amd64 and grigri should be installed manually.</p>"},{"location":"deployment/manual-setup/#amd64-instances","title":"amd64 instances","text":"<p>Installed with Ubuntu: select ubuntu server (non minimized) and follow the process.</p>"},{"location":"deployment/manual-setup/#odroid-hc4","title":"odroid-hc4","text":"<p>Important: To be able to boot clean Armbian mainline based u-boot / kernel experiences, you need to remove incompatible Petitboot loader that is shipped with the board.</p>"},{"location":"deployment/manual-setup/#bootloader-bypass-method","title":"Bootloader Bypass Method","text":"<p>This is now the preferred method. It is easier, and be performed without a display via SSH</p> <p>Install an SD Card with a fresh Armbian image Flip device upside down With a tool, press and hold down the black button. Continue holding button and plug in power to device Login to console or SSH and perform follow normal setup procedures Verify system can access SPI FLASH device and Erase Reboot</p> <pre><code>odroidhc4:~:# ls -ltr /dev/mtd*\ncrw------- 1 root root 90, 0 Nov  6 21:38 /dev/mtd0\nbrw-rw---- 1 root disk 31, 0 Nov  6 21:38 /dev/mtdblock0\ncrw------- 1 root root 90, 1 Nov  6 21:38 /dev/mtd0ro\nodroidhc4:~:# flash_erase /dev/mtd0 0 0\nErasing 4 Kibyte @ fff000 -- 100 % complete\nodroidhc4:~:#\n</code></pre>"},{"location":"deployment/manual-setup/#naming-convention","title":"Naming convention","text":"<p>All nodes must be named with prefix <code>k8s-{hardware_tag}-{numerical_id}</code>. For example:</p> <ul> <li>k8s-odroid-c4-1</li> <li>k8s-amd64-1</li> </ul> <p>Also, consider their use case and performance profile. For example, for Ceph nodes:</p> <ul> <li>k8s-sas-ssd-1</li> <li>k8s-hot-storage-2</li> </ul>"},{"location":"deployment/manual-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/manual-setup/#same-mac-problem","title":"Same mac problem","text":"<p>Editing <code>/boot/ArmbianEnv.txt</code> didn't work.</p> <p><code>/etc/network/interfaces</code>:</p> <pre><code>...\nauto eth0\niface eth0 inet dhcp\n  hwaddress ether b6:09:a4:06:00:8b\n</code></pre>"},{"location":"deployment/manual-setup/#no-python-interpreter-found","title":"No python interpreter found","text":"<pre><code>ln -s /usr/bin/python3 /usr/bin/python\n</code></pre>"},{"location":"deployment/manual-setup/#setup","title":"Setup","text":"<p>Use playbook <code>playbooks/install/so.yml</code> to setup servers.</p> <pre><code>make first-boot\n</code></pre> <p>Note: Armbian default user/password -&gt; root/1234</p>"},{"location":"deployment/manual-setup/#stability-problems-currently-included-400mhz","title":"Stability problems (currently included 400MHz)","text":"<p>Change uboot to use ddr 333 MHz frequency.</p> <p>Compile uboot binaries or download from here</p> <pre><code>git clone https://github.com/armbian/build.git\ncd build\n# edit `config/sources/families/include/rockchip64_common.inc` with this values for rock64:\n#      BOOT_USE_BLOBS=yes\n#      BOOT_SOC=rk3328\n#      DDR_BLOB='rk33/rk3328_ddr_333MHz_v1.16.bin'\n#      MINILOADER_BLOB='rk33/rk322xh_miniloader_v2.50.bin'\n#      BL31_BLOB='rk33/rk322xh_bl31_v1.44.elf'\n\n./compile.sh docker\n# switch to expert (to see rock64) and compile u-boot for rock64\ndocker cp $CONTAINER_ID:/root/armbian/cache/sources/u-boot/v2020.10/uboot.img .\ndocker cp $CONTAINER_ID:/root/armbian/cache/sources/u-boot/v2020.10/idbloader.bin .\ndocker cp $CONTAINER_ID:/root/armbian/cache/sources/u-boot/v2020.10/trust.bin .\n</code></pre> <p>Then, in rock64 SD card:</p> <pre><code>dd if=idbloader.bin of=/dev/mmcblk0 seek=64 conv=notrunc\ndd if=uboot.img of=/dev/mmcblk0 seek=16384 conv=notrunc\ndd if=trust.bin of=/dev/mmcblk0 seek=24576 conv=notrunc\nsync\n</code></pre> <p>Ref:</p> <ul> <li>Stability problems</li> </ul>"},{"location":"deployment/manual-setup/#cluster","title":"Cluster","text":"<p><code>playbooks/install/cluster.yml</code> to setup Kubernetes.</p> <pre><code>ansible-playbook playbooks/install/k8s.yml --become\n</code></pre>"},{"location":"deployment/unifi/","title":"UniFi","text":""},{"location":"deployment/unifi/#config-dns","title":"Config DNS","text":"<ul> <li><code>ServicesDNS-&gt;ResolverGeneral-&gt;Settings-&gt;Host Overrides</code>:</li> </ul> <pre><code>- host: unifi-controller\ndomain: grigri\nip_address: 192.168.193.2\ndescription: Deployed on k8s.grigri in his own load balancer\nadditional_names_for_this_host:\n- host: unifi\ndomain: grigri\ndescription: Used for automatically adopt devices\n</code></pre> <pre><code>\u00bb dog unifi-controller.grigri\nA unifi-controller.grigri. 1h00m00s   192.168.193.2\n\u00bb dog unifi\nA unifi.grigri. 59m50s   192.168.193.2\n</code></pre>"},{"location":"deployment/unifi/#adopt-devices","title":"Adopt devices","text":"<ul> <li>Factory reset:</li> <li>10 seconds button</li> <li>ssh and run <code>syswrapper.sh restore-default</code></li> <li>Connect to the network</li> <li>Click adopt from unifi web interface</li> </ul> <p>Note: For wireless mesh use same channel for both APs (disable Channel Optimization). If still doesn't work go to legacy UI and set up same Channel and width in <code>Settings-&gt;RADIOS</code></p>"},{"location":"deployment/unifi/#ssh-access","title":"ssh access","text":"<p>Add SSH key to <code>System-&gt;Network Device SSH Authentication-&gt;SSH Keys</code></p> <pre><code>ssh -oPubkeyAcceptedKeyTypes=+ssh-rsa -oHostkeyAlgorithms=+ssh-rsa -oKexAlgorithms=+diffie-hellman-group1-sha1 pando85@{{ hostname }}\n</code></pre>"},{"location":"deployment/unifi/#layer-3-methods-for-uap-adoption","title":"Layer 3 methods for UAP adoption","text":"<p>From this doc, using SSH to adopt devices from the DMZ network.</p> <p>After SSH into the device:</p> <pre><code>set-inform http://unifi-controller.grigri:8080/inform\n</code></pre> <p>And then you will see the device now show up for adoption.</p>"},{"location":"network/configuration/","title":"Configuration","text":""},{"location":"network/configuration/#diagram","title":"Diagram","text":""},{"location":"network/configuration/#pfsense","title":"pfSense","text":"<p>Manual configuration and backups in grigri. Check <code>metal/roles/grigri_setup/tasks/backup-user.yml</code> file.</p>"},{"location":"network/configuration/#gs724t","title":"gs724t","text":"<ul> <li><code>Ipv4 Network Interface Configuration -&gt; IP Configuration</code>:</li> </ul> <pre><code>- current_network_configuration_protocol: static\nip_address: 192.168.76.10\nsubnet_mask: 255.255.255.0\ndefault_gateway: 192.168.76.254\n</code></pre> <ul> <li> <p><code>Switching -&gt; VLAN</code>:</p> </li> <li> <p>Add <code>101</code> DMZ</p> </li> <li>Add <code>501</code> IoT</li> <li> <p>Add <code>502</code> IoT Offline</p> </li> <li> <p><code>Switching -&gt; VLAN -&gt; Advanced -&gt; VLAN Membership</code>:</p> </li> </ul> <pre><code>VLAN ID: 1\nVLAN Name: Default\nVLAN Type: Default\nPort  1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n                                                         U    U    U    U    U    U    U    U    U    U    U    U\n</code></pre> <pre><code>VLAN ID: 101\nVLAN Name: DMZ\nVLAN Type: Static\nPort  1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n      U   U   U   U   U   U   U   U   U   U    U    U\n</code></pre> <pre><code>VLAN ID: 501\nVLAN Name: IoT\nVLAN Type: Static\nPort  1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n                                                                                                      T    T    T\n</code></pre> <pre><code>VLAN ID: 502\nVLAN Name: IoT Offline\nVLAN Type: Static\nPort  1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24\n                                                                                                      T    T    T\n</code></pre> <ul> <li><code>Switching -&gt; VLAN -&gt; Advanced -&gt; Port PVID Configuration</code>:</li> </ul> <pre><code>g1   101   101   101   None   Admit All   Disable   Disable   0\ng2   101   101   101   None   Admit All   Disable   Disable   0\ng3   101   101   101   None   Admit All   Disable   Disable   0\ng4   101   101   101   None   Admit All   Disable   Disable   0\ng5   101   101   101   None   Admit All   Disable   Disable   0\ng6   101   101   101   None   Admit All   Disable   Disable   0\ng7   101   101   101   None   Admit All   Disable   Disable   0\ng8   101   101   101   None   Admit All   Disable   Disable   0\ng9   101   101   101   None   Admit All   Disable   Disable   0\ng10   101   101   101   None   Admit All   Disable   Disable   0\ng11   101   101   101   None   Admit All   Disable   Disable   0\ng12   101   101   101   None   Admit All   Disable   Disable   0\ng13   1   1   1   None   Admit All   Disable   Disable   0\ng14   1   1   1   None   Admit All   Disable   Disable   0\ng15   1   1   1   None   Admit All   Disable   Disable   0\ng16   1   1   1   None   Admit All   Disable   Disable   0\ng17   1   1   1   None   Admit All   Disable   Disable   0\ng18   1   1   1   None   Admit All   Disable   Disable   0\ng19   1   1   1   None   Admit All   Disable   Disable   0\ng20   1   1   1   None   Admit All   Disable   Disable   0\ng21   1   1   1   None   Admit All   Disable   Disable   0\ng22   1   1   1,501-502   501-502   Admit All   Disable   Disable   0\ng23   1   1   1   None   Admit All   Disable   Disable   0\ng24   1   1   1,101,501-502   101,501-502   Admit All   Disable   Disable   0\ng25   1   1   1   None   Admit All   Disable   Disable   0\ng26   1   1   1   None   Admit All   Disable   Disable   0\n</code></pre> <p>Summary:</p> <pre><code>port 1-12: DMZ (VLAN 101)\nport 13-21: LAN (VLAN 1/default)\nport 22-23: LAN (VLAN 1 - Untagged, VLAN 501 - Tagged, VLAN 502 - Tagged)\nport 24: LAN (VLAN 1 - Untagged, VLAN101 - Tagged, VLAN 501 - Tagged, VLAN 502 - Tagged)\n</code></pre>"},{"location":"troubleshooting/major-page-faults/","title":"Major page faults","text":""},{"location":"troubleshooting/major-page-faults/#summary","title":"Summary","text":"<p><code>grigri</code> server is suffering a high rate of major page faults since it was reinstalled.</p>"},{"location":"troubleshooting/major-page-faults/#timeline","title":"Timeline","text":"<p>2023-08-21:</p> <pre><code>- 22:57:XX CEST - Ubuntu 22.04 installed.\n- 23:34:07 CEST - Added to the cluster as worker.\n</code></pre> <p>2023-08-22:</p> <pre><code>- 06:XX:XX CEST - Max ~500 major page faults per second. 95% of memory utilization.\n- 18:40:XX CEST - `grigri` becomes K3s server.\n</code></pre> <p>2023-09-05:</p> <pre><code>- Reduce `zfs_arc_max` to keep under 90% of memory usaged. This fixed major page faults.\n</code></pre> <p>2023-09-06:</p> <pre><code>- 09:14:XX CEST - Increase `zfs_arc_max` and set up `zfs_arc_sys_free` (3.2Gi) to try to avoid having free memory but still ensuring that major page faults are not happening any more.\n- 12:30:XX CEST - Increase `zfs_arc_sys_free` to 4Gi cause memory available is ~ 1.5Gi.\n- 16:22:XX CEST - Remove `zfs_arc_sys_free` and decrease `zfs_arc_max` to 12Gi.\n</code></pre>"},{"location":"troubleshooting/major-page-faults/#root-cause-analysis","title":"Root cause analysis","text":"<p>High memory usage is causing this major page faults. We still have to investigate why 95% seems the threshold for this system.</p>"},{"location":"troubleshooting/mqtt/","title":"MQTT","text":"<p>Connect to server mosquitto with web client:</p> <pre><code>docker run --rm --name mqttx-web -p 80:80 emqx/mqttx-web\n</code></pre> <pre><code>name: random\nclient_id: random\nhost: mosquitto.internal.grigri.cloud\nport: 8083\npath: /mqtt\nusername: vault:mosquitto/user#username\npassword: vault:mosquitto/user#password\n</code></pre> <p>Subscribe to all topics with <code>#</code>.</p>"},{"location":"user-guide/add-or-remove-nodes/","title":"Add or remove nodes","text":"<p>Or how to scale vertically. To replace the same node with a clean OS, remove it and add it again.</p>"},{"location":"user-guide/add-or-remove-nodes/#add-new-nodes","title":"Add new nodes","text":"<p>Tip</p> <p>You can add multiple nodes at the same time</p> <p>Add nodes details to the inventory at the end of the group (kube_control_plane or kube_node):</p> <pre><code>diff --git a/metal/inventories/master/inventory.ini b/metal/inventories/master/inventory.ini\nindex fe0ca8b..ddbca8d 100644\n--- a/metal/inventories/master/inventory.ini\n+++ b/metal/inventories/master/inventory.ini\n@@ -18,6 +18,7 @@ odroid-hc4\nk8s-odroid-hc4-1\n k8s-odroid-hc4-2\n k8s-odroid-hc4-3\n+k8s-odroid-hc4-4\n\n[amd64]\n k8s-amd64-1\n</code></pre> <p>Setup OS and network: manual Setup</p> <p>Join the cluster:</p> <pre><code>make metal\n</code></pre> <p>That's it!</p>"},{"location":"user-guide/add-or-remove-nodes/#remove-a-node","title":"Remove a node","text":"<p>Danger</p> <p>It is recommended to remove nodes one at a time</p> <p>Remove it from the inventory:</p> <pre><code>diff --git a/metal/inventories/master/inventory.ini b/metal/inventories/master/inventory.ini\nindex fe0ca8b..19891bf 100644\n--- a/metal/inventories/master/inventory.ini\n+++ b/metal/inventories/master/inventory.ini\n@@ -17,7 +17,6 @@ odroid-hc4\n[odroid-hc4]\n k8s-odroid-hc4-1\n k8s-odroid-hc4-2\n-k8s-odroid-hc4-3\n\n[amd64]\n k8s-amd64-1\n</code></pre> <p>Drain the node:</p> <pre><code>kubectl drain ${NODE_NAME} --delete-emptydir-data --ignore-daemonsets --force\n</code></pre> <p>Remove the node from the cluster</p> <pre><code>kubectl delete node ${NODE_NAME}\n</code></pre> <p>Shutdown the node:</p> <pre><code>ssh root@${NODE_IP} poweroff\n</code></pre>"},{"location":"user-guide/clone-data/","title":"Clone data between volumes","text":""},{"location":"user-guide/clone-data/#rsync","title":"rsync","text":"<p>No dependencies:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\nname: rsync\nnamespace: plex\nspec:\ntemplate:\nmetadata:\nname: rsync\nspec:\ncontainers:\n- name: rsync\nimage: instrumentisto/rsync-ssh\ncommand:\n- rsync\n- -av\n- --numeric-ids\n- /src/\n- /dest/\nvolumeMounts:\n- name: src\nmountPath: \"/src/\"\n- name: dest\nmountPath: \"/dest/\"\nvolumes:\n- name: src\npersistentVolumeClaim:\nclaimName: config-plex-0\n- name: dest\npersistentVolumeClaim:\nclaimName: config-plex-0-zfs\nrestartPolicy: Never\nnodeSelector:\nname: grigri\n</code></pre>"},{"location":"user-guide/expand-longhorn-volume/","title":"Expand Longhorn volume","text":"<p>Longhorn requires volumes to be detached in order to expand them.</p> <p>An example of volume expansion:</p> <pre><code>kubectl -n gitea delete --cascade=orphan sts gitea-postgres\nkubectl -n gitea delete pod gitea-postgres-0\nkubectl -n gitea patch pvc pgdata-gitea-postgres-0 -p '{ \"spec\": { \"resources\": { \"requests\": { \"storage\": \"1.5Gi\" }}}}'\n</code></pre>"},{"location":"user-guide/freshrss/","title":"FreshRSS","text":""},{"location":"user-guide/freshrss/#sqlite-backup","title":"SQLite Backup","text":""},{"location":"user-guide/freshrss/#export","title":"Export","text":"<p>To export data from FreshRSS using SQLite, utilize the following command:</p> <pre><code>./cli/export-sqlite-for-user.php --user ${USERNAME} --filename /tmp/freshrss.sqlite\n</code></pre>"},{"location":"user-guide/freshrss/#import","title":"Import","text":"<p>To import previously exported SQLite data back into FreshRSS, use the following command:</p> <pre><code>./cli/import-sqlite-for-user.php --user ${USERNAME} --force-overwrite --filename /tmp/freshrss.sqlite\n</code></pre>"},{"location":"user-guide/freshrss/#postgresql-configuration","title":"PostgreSQL Configuration","text":"<p>Optimizing Full-text search in PostgreSQL for FreshRSS involves adding indexes. This can significantly improve search performance without modifying FreshRSS' code (which uses ILIKE).</p> <p>First, ensure you have the <code>pg_trgm</code> extension installed:</p> <pre><code>CREATE EXTENSION pg_trgm;\n</code></pre> <p>Then, create the necessary indexes for title and content:</p> <pre><code>CREATE INDEX gin_trgm_index_title ON \"freshrss_entry\" USING gin(title gin_trgm_ops);\nCREATE INDEX gin_trgm_index_content ON \"freshrss_entry\" USING gin(content gin_trgm_ops);\n</code></pre> <p>Replace \"freshrss_entry\" with the appropriate entry name (e.g., freshrss_alice_entry).</p> <p>For faster searches on authors (e.g., author:Alice), add another index:</p> <pre><code>CREATE INDEX gin_trgm_index_author ON freshrss_entry USING gin(author gin_trgm_ops);\n</code></pre> <p>Repeat this process for other text fields as needed. Refer to the CREATE TABLE _entry section for the list of fields.</p>"},{"location":"user-guide/freshrss/#references","title":"References","text":"<ul> <li>FreshRSS Database Configuration</li> </ul>"},{"location":"user-guide/grigri/","title":"grigri","text":"<p>Motherboard: Supermicro A1SRi-2758F</p>"},{"location":"user-guide/grigri/#remote-access","title":"Remote access","text":"<p>Go to https://grigri-ipmi.grigri: <code>Remote Control -&gt; Console Redirection</code></p> <pre><code>sudo archlinux-java set java-8-openjdk\njavaws /tmp/launch.jnlp\n</code></pre>"},{"location":"user-guide/import_zfs_dataset/","title":"Import ZFS dataset","text":""},{"location":"user-guide/import_zfs_dataset/#steps","title":"Steps","text":"<ul> <li>stop app</li> <li>clone volume</li> <li>create new volume</li> <li>change volume for app</li> <li>check app</li> <li>remove old volume</li> </ul>"},{"location":"user-guide/import_zfs_dataset/#commands","title":"Commands","text":"<p>In ZFS server:</p> <pre><code># variables\nPVC=datasets/k8s/l/v/pvc-0f17e0bf-6741-44fa-9e37-e5ed394ff56b\nNAME=transcoder-rabbit\nSIZE=1G\nNAMESPACE=transcoder\n\nPVC_NAME=${NAME}\nDATASET=datasets/openebs\nNEW_DATASET=${DATASET}/${NAME}\n\nzfs snapshot ${PVC}@clone\nzfs clone ${PVC}@clone ${NEW_DATASET}\nzfs promote ${NEW_DATASET}\nzfs unmount ${NEW_DATASET}\nzfs set mountpoint=legacy ${NEW_DATASET}\nzfs set quota=${SIZE} ${NEW_DATASET}\n\ncat &lt;&lt; EOF &gt; /tmp/pv.yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: ${NAME}\nspec:\n  accessModes:\n  - ReadWriteOnce\n  capacity:\n    storage: ${SIZE}i # size of the volume\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: ${PVC_NAME}\n    namespace: ${NAMESPACE}\n  csi:\n    driver: zfs.csi.openebs.io\n    fsType: zfs\n    volumeAttributes:\n      openebs.io/poolname: ${DATASET}\n    volumeHandle: ${NAME}\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - grigri\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: openebs-zfspv\n  volumeMode: Filesystem\nEOF\nk3s kubectl apply -f /tmp/pv.yaml\n\ncat &lt;&lt; EOF &gt; /tmp/zfs-volume.yaml\napiVersion: zfs.openebs.io/v1\nkind: ZFSVolume\nmetadata:\n  finalizers:\n  - zfs.openebs.io/finalizer\n  name: ${NAME}\n  namespace: zfs-localpv\nspec:\n  capacity: \"$(echo \"(${SIZE::-1} * 1024 * 1024 * 1024) / 1\" | bc)\" # size of the volume in bytes\n  fsType: zfs\n  ownerNodeID: grigri\n  shared: \"yes\"\n  poolName: ${DATASET}\n  volumeType: DATASET\nstatus:\n  state: Ready\nEOF\nk3s kubectl apply -f /tmp/zfs-volume.yaml\n</code></pre>"},{"location":"user-guide/install-pre-commit-hooks/","title":"Install pre-commit hooks","text":"<p>Git hook scripts are useful for identifying simple issues before commiting changes.</p> <p>Install pre-commit first, one-liner for Arch users:</p> <pre><code>sudo pacman -S python-pre-commit\n</code></pre> <p>Then install git hook scripts:</p> <pre><code>make git-hooks\n</code></pre>"},{"location":"user-guide/kanidm/","title":"Kanidm","text":""},{"location":"user-guide/kanidm/#ldap-connection","title":"LDAP connection","text":"<p>For login with LDAP accounts your user has to has enabled the POSIX attributes and need to set a Unix password.</p>"},{"location":"user-guide/kanidm/#modify-acp","title":"Modify ACP","text":"<p>Disallow displayname self modification:</p> <pre><code>cat &lt;&lt; EOF&gt; /tmp/modify.json\n[\n    { \"removed\": [\"acp_modify_removedattr\", \"displayname\"] },\n    { \"removed\": [\"acp_modify_presentattr\", \"displayname\"] }\n]\nEOF\nkanidm raw modify '{\"eq\": [\"name\", \"idm_self_acp_write\"]}'  /tmp/modify.json\nkanidm raw search '{\"eq\": [\"name\", \"idm_self_acp_write\"]}'\n</code></pre>"},{"location":"user-guide/kanidm/#create-user","title":"Create user","text":"<pre><code>kanidm person create demo-user \"demo-user\" -D idm_admin\nkanidm person update demo-user --mail \"demo-user@example.com\" -D idm_admin\nkanidm person credential create-reset-token demo-user -D idm_admin\n\nkanidm group list -D idm_admin | rg name | rg users\nkanidm group add-members ${GROUP_NAME} demo-user -D idm_admin\n</code></pre>"},{"location":"user-guide/kanidm/#add-app-to-sso","title":"Add app to SSO","text":"<p>Example with Grafana:</p> <pre><code>kanidm system oauth2 create grafana grafana  https://grafana.grigri.cloud/login/generic_oauth -D admin\nkanidm group create grafana-users -D admin\nkanidm group add-members grafana-users ${USER} -D admin\nkanidm system oauth2 update-scope-map grafana grafana-users openid profile email -D admin\nkanidm system oauth2 show-basic-secret grafana -D admin\nkanidm group create grafana-admins -D admin\nkanidm group add-members grafana-admins ${USER} -D admin\nkanidm system oauth2 update-sup-scope-map grafana grafana-admins admin -D admin\nkanidm system oauth2 prefer-short-username grafana -D admin\nkanidm system oauth2 set-landing-url grafana https://grafana.grigri.cloud/login/generic_oauth\n</code></pre> <p>If PKCE needs to be disabled:</p> <pre><code>kanidm system oauth2 warning-insecure-client-disable-pkce ${CLIENT}\n</code></pre>"},{"location":"user-guide/kubernetes-upgrade/","title":"Kubernetes upgrade","text":"<p>Update <code>k3s_version</code> to desired version and then run:</p> <pre><code>cd metal\nANSIBLE_EXTRA_ARGS=\"-t k3s-upgrade -e serial=1\" make cluster\n</code></pre> <p>Note: it worked perfectly with serial=100% or running it by default.</p>"},{"location":"user-guide/ldap/","title":"LDAP","text":""},{"location":"user-guide/ldap/#deployment","title":"Deployment","text":"<p>The deployment of the Lightweight Directory Access Protocol (LDAP) application was carried out in k8s using the Fedora guide for OpenShift as reference.</p>"},{"location":"user-guide/ldap/#migration","title":"Migration","text":"<p>The migration process involved recreating the backend and suffix, as follows:</p> <pre><code>dsconf localhost backend create --suffix dc=grigri,dc=cloud --be-name userroot --create-suffix\n</code></pre> <p>To import current values, LDIF files were utilized. First, the current LDIF files were exported using the following command:</p> <pre><code>ldapsearch -w 'XXXXXXX' -D 'cn=Directory Manager' -H ldaps://ldap.grigri -b dc=grigri,dc=cloud -LLL \"(objectclass=*)\"\n</code></pre> <p>Next, Apache Directory Studio was used for importing the exported files.</p>"},{"location":"user-guide/ldap/#permissions","title":"Permissions","text":"<p>The permissions for the LDAP deployment were set in accordance with the guidelines provided by Red Hat's official documentation. The following commands were executed:</p> <pre><code>ldapmodify -w 'XXXXXXX' -D 'cn=Directory Manager' -H ldaps://ldap.grigri -b dc=grigri,dc=cloud -x\ndn: dc=grigri,dc=cloud\nchangetype: modify\nadd: aci\naci: (targetattr!=\"userPassword || aci\")(version 3.0; acl \"Enable anonymous ac\n cess\"; allow (read, search, compare) userdn=\"ldap:///anyone\";)\n\nldapmodify -w 'XXXXXXX' -D 'cn=Directory Manager' -H ldaps://ldap.grigri -b dc=grigri,dc=cloud -x\ndn: ou=People,dc=grigri,dc=cloud\nchangetype: modify\nadd: aci\naci: (targetattr=\"userpassword\")(version 3.0; acl \"read password access\"; allo\n w(read) userdn =  \"ldap:///uid=*,ou=Read,ou=ServiceUsers,dc=grigri,dc=cloud\";\n)\n</code></pre>"},{"location":"user-guide/ldap/#references","title":"References","text":""},{"location":"user-guide/migrate_controller_node/","title":"Migrate controller node","text":"<p>Based in K3s backup and restore doc.</p>"},{"location":"user-guide/migrate_controller_node/#procedure","title":"Procedure","text":"<ol> <li>Take etcd snapshot and stop k3s in old controller.</li> <li>Copy <code>/var/lib/rancher/k3s/server</code> and etcd snapshot to new controller.</li> <li>Update Ansible inventory controller node and start new controller.</li> <li>Check API is working and there are no agents.</li> <li>Start all agents with <code>make cluster</code>.</li> </ol> <p>Example:</p> <pre><code># k8s-amd64-1\nk3s etcd-snapshot save\nsystemctl stop k3s\n\nrsync -av --delete -e \"ssh -i /root/id_ed25519\" /var/lib/rancher/k3s/server/ \\\nbackup@grigri.grigri:/datasets/backups/k8s/server\nrsync -av -e \"ssh -i /root/id_ed25519\" \\\n/var/lib/rancher/k3s/server/db/snapshots/on-demand-k8s-amd64-1-1692721476 \\\nbackup@grigri.grigri:/datasets/backups/k8s/\n\n# grigri\nk3s server --cluster-reset --cluster-reset-restore-path=/datasets/backups/k8s/on-demand-k8s-amd64-1-1692721476 --token /datasets/backups/k8s/server/token\n\n# localhost\ncd metal\nANSIBLE_EXTRA_ARGS=\"--limit grigri\" make cluster\nkubectl get nodes\nmake cluster\n</code></pre>"},{"location":"user-guide/nextcloud/","title":"Nextcloud","text":"<ul> <li>Migrate auth to OIDC</li> <li>Migrate Nextcloud</li> <li>Nextcloud<ul> <li>apps</li> <li>data</li> </ul> </li> <li>Postgres</li> <li>occ command</li> <li>Troubleshooting</li> <li>Not automatically upgraded</li> <li>Automatically upgrades fails</li> </ul>"},{"location":"user-guide/nextcloud/#migrate-auth-to-oidc","title":"Migrate auth to OIDC","text":"<ul> <li>Configure Kanidm oauth2 with   OpenID Connect user backend for Nextcloud.</li> <li>Follow Kanidm guide for   Nextcloud config.</li> <li>Use<code>displayname</code> from Kanidm (<code>name</code> in OIDC JWT) as user ID and keep user IDs from LDAP auth.</li> </ul>"},{"location":"user-guide/nextcloud/#migrate-nextcloud","title":"Migrate Nextcloud","text":""},{"location":"user-guide/nextcloud/#nextcloud_1","title":"Nextcloud","text":""},{"location":"user-guide/nextcloud/#apps","title":"apps","text":"<pre><code>POD_NAME=$(KUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud get pod -l app.kubernetes.io/component=app --no-headers -o custom-columns=\":metadata.name\")\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud cp config/www/nextcloud/apps/ $POD_NAME:/tmp/\n\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud exec -it $POD_NAME -- bash\nrm -rf /var/www/html/apps\nmv /tmp/apps /var/www/html/apps\nchown -R www-data:www-data /var/www/html/apps\n</code></pre>"},{"location":"user-guide/nextcloud/#data","title":"data","text":"<pre><code>POD_NAME=$(KUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud get pod -l app.kubernetes.io/component=app --no-headers -o custom-columns=\":metadata.name\")\nfor DIR in $(find /datasets/nextcloud/ -maxdepth 1 -mindepth 1 -not -path '*/teresa' -not -path '*/pando' -not -path '*/billee'); do\necho $DIR;\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud cp --retries=5 $DIR $POD_NAME:/var/www/html/data/\ndone\n#KUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud cp --retries=5 /datasets/fotos $POD_NAME:/var/www/html/data/pando/files/\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud exec -it $POD_NAME -- bash\nchown -R www-data:www-data /var/www/html/data\n</code></pre> <p>Alternative using rsync:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\nname: rsync\nnamespace: nextcloud\nspec:\ntemplate:\nmetadata:\nname: rsync\nspec:\ncontainers:\n- name: rsync\nimage: instrumentisto/rsync-ssh\ncommand:\n- rsync\n- -avz\n- --numeric-ids\n- --delete\n- --progress\n- /src/\n- /dest/data/\nvolumeMounts:\n- name: src\nmountPath: \"/src/\"\n- name: dest\nmountPath: \"/dest/\"\nvolumes:\n- name: src\nhostPath:\npath: /datasets/nextcloud/\ntype: Directory\n- name: dest\npersistentVolumeClaim:\nclaimName: nextcloud-nextcloud-data\nrestartPolicy: Never\nnodeSelector:\nname: grigri\n</code></pre>"},{"location":"user-guide/nextcloud/#postgres","title":"Postgres","text":"<pre><code>docker exec -it nextcloud_db_1 bash\n\nPGPASSWORD=\"$POSTGRES_PASSWORD\" pg_dump \"$POSTGRES_DB\" -h localhost -U \"$POSTGRES_USER\" -f /tmp/nextcloud-sqlbkp.bak\nexit\ndocker cp nextcloud_db_1:/tmp/nextcloud-sqlbkp.bak /tmp/\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud cp /tmp/nextcloud-sqlbkp.bak nextcloud-postgres-0:/tmp/\n\nKUBECONFIG=/tmp/kubeconfig.yaml kubectl -n nextcloud exec -it nextcloud-postgres-0 -- bash\nsu - postgres\npsql -d template1 -c \"DROP DATABASE \\\"nextcloud\\\";\"\npsql -d template1 -c \"CREATE DATABASE \\\"nextcloud\\\";\"\npsql -d nextcloud -f /tmp/nextcloud-sqlbkp.bak\n</code></pre>"},{"location":"user-guide/nextcloud/#occ-command","title":"occ command","text":"<p>Run with kubectl:</p> <pre><code>chsh -s /bin/bash www-data\nsu - www-data\n/var/www/html/occ\n</code></pre>"},{"location":"user-guide/nextcloud/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/nextcloud/#not-automatically-upgraded","title":"Not automatically upgraded","text":"<pre><code>chsh -s /bin/bash www-data\nsu - www-data\n/var/www/html/occ upgrade\n</code></pre>"},{"location":"user-guide/nextcloud/#automatically-upgrades-fails","title":"Automatically upgrades fails","text":"<p>Check <code>https://nextcloud.grigri.cloud/index.php/settings/integrity/failed</code>.</p> <p>Always fails in code integrity check without any errors, just hangs forever in the web browser auto updater.</p> <pre><code>chsh -s /bin/bash www-data\nsu - www-data\n/var/www/html/occ maintenance:mode\n</code></pre>"},{"location":"user-guide/restore-backup/","title":"Restore backup","text":""},{"location":"user-guide/restore-backup/#zfs","title":"ZFS","text":"<pre><code>zfs rollback ${ZFS_VOLUME}@{ZFS_SNAPSHOT}\n</code></pre>"},{"location":"user-guide/restore-backup/#postgres","title":"Postgres","text":"<p>If after restore there are this kind of errors:</p> <p>My wal position exceeds maximum replication lag</p> <p>Replica can be promoted to leader using <code>patronictl failover</code> command as root.</p>"},{"location":"user-guide/restore-backup/#velero","title":"Velero","text":""},{"location":"user-guide/restore-backup/#restoring-to-a-different-node","title":"Restoring to a Different Node","text":"<p>To restore to a different node, create a ConfigMap with the following YAML:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n# any name can be used; Velero uses the labels (below)\n# to identify it rather than the name\nname: change-pvc-node-selector-config\n# must be in the velero namespace\nnamespace: velero\n# the below labels should be used verbatim in your\n# ConfigMap.\nlabels:\n# this value-less label identifies the ConfigMap as\n# config for a plugin (i.e. the built-in restore item action plugin)\nvelero.io/plugin-config: \"\"\n# this label identifies the name and kind of plugin\n# that this ConfigMap is for.\nvelero.io/change-pvc-node-selector: RestoreItemAction\ndata:**Warning**: ArgoCD labels are going to be restored too. If you are doing the restore into other namespace as in the example, disable ArgoCD or modify the labels.\n# add 1+ key-value pairs here, where the key is the old\n# node name and the value is the new node name.\ngrigri: k8s-amd64-1\n</code></pre>"},{"location":"user-guide/restore-backup/#restoring-with-a-different-storage-class","title":"Restoring with a Different Storage Class","text":"<p>To restore with a different storage class, create a ConfigMap with the following YAML:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n# any name can be used; Velero uses the labels (below)\n# to identify it rather than the name\nname: change-storage-class-config\n# must be in the velero namespace\nnamespace: velero\n# the below labels should be used verbatim in your\n# ConfigMap.\nlabels:\n# this value-less label identifies the ConfigMap as\n# config for a plugin (i.e. the built-in restore item action plugin)\nvelero.io/plugin-config: \"\"\n# this label identifies the name and kind of plugin\n# that this ConfigMap is for.\nvelero.io/change-storage-class: RestoreItemAction\ndata:\n# add 1+ key-value pairs here, where the key is the old\n# storage class name and the value is the new storage\n# class name.\nopenebs-zfspv: backup\n</code></pre> <p>Warning: Ensure that the original parent zfs volume exists in the target storage class (e.g., <code>datasets/openebs</code>).</p> <pre><code>velero restore create --from-backup ${BACKUP_NAME} --include-namespaces ${NAMESPACE} --restore-volumes=true --namespace-mappings ${NAMESPACE}:${TARGET_NAMESPACE}\n</code></pre> <p>Note: ArgoCD labels will also be restored. If restoring to a different namespace, consider disabling ArgoCD or modifying the labels accordingly.</p>"},{"location":"user-guide/run-commands-on-multiple-nodes/","title":"Run commands on multiple nodes","text":"<p>Use ansible-console:</p> <pre><code>cd metal\nmake console\n</code></pre> <p>Then enter the command(s) you want to run.</p> <p>Example</p> <p><code>root@all (4)[f:5]$ uptime</code></p> <pre><code>metal0 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.17, 0.15, 0.06\nmetal1 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.14, 0.11, 0.04\nmetal3 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.03, 0.02, 0.00\nmetal2 | CHANGED | rc=0 &gt;&gt;\n 10:52:02 up 2 min,  1 user,  load average: 0.06, 0.06, 0.02\n</code></pre>"},{"location":"user-guide/upgrades/","title":"Upgrades","text":""},{"location":"user-guide/upgrades/#os-upgrades","title":"OS upgrades","text":"<p>Managed by <code>unattended-upgrade</code> in Debian based distributions and rebooted by <code>kured</code> when needed.</p> <p>Review the update history in <code>/var/log/unattended-upgrades/unattended-upgrades-dpkg.log</code></p>"},{"location":"user-guide/upgrades/#k3s-upgrades","title":"k3s upgrades","text":"<p>Managed by system-upgrade-controller. Increase K3s version in <code>system/system-upgrade/k3s/kustomization.yaml</code> file.</p>"},{"location":"user-guide/valetudo-upgrade/","title":"Valetudo upgrade","text":""},{"location":"user-guide/valetudo-upgrade/#operation","title":"Operation","text":"<pre><code>ssh root@tanque.iot.grigri\nkillall valetudo\nwget https://github.com/Hypfer/Valetudo/releases/latest/download/valetudo-aarch64 -O /data/valetudo\nreboot\n</code></pre>"},{"location":"user-guide/valetudo-upgrade/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/valetudo-upgrade/#text-file-busy-error","title":"\"Text file busy\" error","text":"<p>Valetudo is still running. Try to kill it again.</p> <p>If the issue still occurs, delete the old binary before uploading the new one.</p>"},{"location":"user-guide/wallabag/","title":"Wallabag","text":""},{"location":"user-guide/wallabag/#fix-user-login-fail","title":"Fix user login fail","text":"<pre><code>cd /var/www/wallabag\nsu -c \"php bin/console cache:clear --env=prod\" -s /bin/sh nobody\nsu -c \"php bin/console doctrine:migrations:migrate --no-interaction --env=prod\" -s /bin/sh nobody\n</code></pre>"},{"location":"user-guide/wan-phone-failover/","title":"WAN phone failover","text":"<p>Use phone as WAN failover mechanism.</p>"},{"location":"user-guide/wan-phone-failover/#steps","title":"Steps","text":"<p>Connect phone to router. In the phone:</p> <pre><code>Ajustes -&gt; Punto de acces port\u00e1til -&gt; Anclaje USB\n</code></pre> <p>In the router:</p> <pre><code>Interfaces -&gt; Add ue8 -&gt; DHCP\nSystem -&gt; Routing ...\n</code></pre>"}]}